{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from unicode_tr import unicode_tr\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from math import log, inf\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=TurkishStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findfiles(path,flist):\n",
    "    dirs = os.listdir(path)\n",
    "    for df in dirs:\n",
    "        if os.path.isdir(path+\"/\"+df):\n",
    "            findfiles(path+\"/\"+df,flist)\n",
    "        else:\n",
    "            flist.append(path+\"/\"+df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './42bin_haber/news'\n",
    "categories = os.listdir(path)[1:]\n",
    "categories = ['ekonomi','kultur-sanat','magazin','saglik','siyaset','spor','teknoloji']\n",
    "news_files = {}\n",
    "for cat in categories:\n",
    "    flist = []\n",
    "    findfiles(path+\"/\"+cat,flist)\n",
    "    news_files[cat] = flist[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words,stop_words = stopwords.words('turkish'),url_regex=None):\n",
    "    #Remove URLS\n",
    "    if url_regex:\n",
    "        words = [word for word in words if not re.match(url_regex,word)]\n",
    "\n",
    "    #Remove trash characters\n",
    "    words = [re.sub(\"\\xad|\\x95|\\x80|\\x82|\\x93|\\x94|\\x91|\\x92|\\x96|^\\'+|^\\*+|^-+|\\'+$\", \"\", word) for word in words]\n",
    "\n",
    "    #Remove nonalphanumeric\n",
    "    words = [word for word in words if not re.match(\"\\W\", word)]\n",
    "\n",
    "    #Lower all words\n",
    "    words = [unicode_tr(word.strip()).lower() for word in words if word.strip()!=\"\"]\n",
    "    \n",
    "    #Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    #Stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_corpus = {}\n",
    "test_files = {}\n",
    "all_words = []\n",
    "X = []\n",
    "y = []\n",
    "for cat in categories:\n",
    "    for f in news_files[cat]:\n",
    "        with open(f) as file:\n",
    "            lines = file.readlines()\n",
    "            lines = ''.join(lines)\n",
    "\n",
    "            words = word_tokenize(lines)\n",
    "            words = preprocess(words)\n",
    "\n",
    "            X.append(words)\n",
    "            y.append(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold partitioning with k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_together(x,y):\n",
    "    together = list(zip(x,y))\n",
    "    shuffle(together)\n",
    "    x,y = zip(*together)\n",
    "    return list(x),list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "X_trains =[]\n",
    "X_tests =[]\n",
    "y_trains = []\n",
    "y_tests = []\n",
    "X_shuffled, y_shuffled = shuffle_together(X,y)\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    test_start = int(i*len(X_shuffled)/5)\n",
    "    test_end = int((i+1)*len(X_shuffled)/5)\n",
    "    \n",
    "    X_train = X_shuffled[:test_start]+X_shuffled[test_end:]\n",
    "    X_test = X_shuffled[test_start:test_end]\n",
    "    y_train = y_shuffled[:test_start]+y_shuffled[test_end:]\n",
    "    y_test = y_shuffled[test_start:test_end]\n",
    "    \n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "    y_trains.append(y_train)\n",
    "    y_tests.append(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams(words):\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_prob(test_words, class_words, class_prob):\n",
    "    prob = 0\n",
    "    total = class_words['Total']\n",
    "    unique = len(class_words.keys())-1\n",
    "    for word in test_words:\n",
    "        # calculates p(word|class) with Laplace Smoothing\n",
    "        prob += log((class_words[word]+1)/(total+unique))\n",
    "      \n",
    "    # This line considers class probability p(class)\n",
    "    prob += log(class_prob)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in0\n",
      "out0\n",
      "in1\n",
      "out1\n",
      "in2\n",
      "out2\n",
      "in3\n",
      "out3\n",
      "in4\n",
      "out4\n"
     ]
    }
   ],
   "source": [
    "naive_results = []\n",
    "\n",
    "# K-fold training\n",
    "for i,X_train in enumerate(X_trains):\n",
    "    print('in'+str(i))\n",
    "    y_train = y_trains[i]\n",
    "    \n",
    "    # class probabilities\n",
    "    P={cat: y_train.count(cat) for cat in categories}\n",
    "    total = len(y_train)\n",
    "    P={cat: P[cat]/total for cat in categories}\n",
    "    \n",
    "    category_corpus = {}\n",
    "    for cat in categories:\n",
    "        category_corpus[cat] = []\n",
    "        \n",
    "    for j,x in enumerate(X_train):\n",
    "        category_corpus[y_train[j]] += x\n",
    "\n",
    "    category_unigrams = {}\n",
    "    for cat in categories:\n",
    "        category_unigrams[cat] = get_unigrams(category_corpus[cat])\n",
    "        category_unigrams[cat]['Total'] = sum(category_unigrams[cat].values())\n",
    "    \n",
    "    X_test = X_tests[i]\n",
    "    y_test = y_tests[i]\n",
    "    predictions= []\n",
    "    print('out'+str(i))\n",
    "    for x in X_test:        \n",
    "        max_prob = -inf\n",
    "        pred_cat = \"\"\n",
    "        for cat in categories:\n",
    "            pred_prob = get_conditional_prob(x, category_unigrams[cat], P[cat])\n",
    "            if pred_prob > max_prob:\n",
    "                max_prob = pred_prob\n",
    "                pred_cat = cat\n",
    "\n",
    "        predictions.append(pred_cat)\n",
    "    \n",
    "    # Append accuracy of i'th fold\n",
    "    naive_results.append(sum(1 for x,y in zip(y_test, predictions) if x == y) / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90591841546805, 0.9217165762791795, 0.9198302287196416, 0.9207734024994105, 0.9198302287196416]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9176137703371847"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(naive_results)\n",
    "np.mean(naive_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(X):\n",
    "    frequency = defaultdict(int)\n",
    "    for x in X:\n",
    "        for token in x:\n",
    "            frequency[token] +=1\n",
    "\n",
    "    texts = [ [token for token in doc if frequency[token] > 1] \n",
    "            for doc in X]\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_doc(texts):\n",
    "    averaged_doc = []\n",
    "    shape = word_vectors.get_vector(texts[0][0]).shape\n",
    "    new_words = []\n",
    "    for doc in texts:\n",
    "        temp = np.zeros(shape)\n",
    "        counter = 0\n",
    "        for token in doc:\n",
    "            try:\n",
    "                temp += word_vectors.get_vector(token)\n",
    "            except:\n",
    "                counter +=1\n",
    "        \n",
    "        new_words.append(counter)\n",
    "        temp = temp/(len(doc)-counter)\n",
    "        averaged_doc.append(temp)\n",
    "    \n",
    "    return np.array(averaged_doc),new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(averaged,model,y):\n",
    "    predictions = model.predict(averaged)\n",
    "    _predictions = (predictions.argmax(1)[:,None] == np.arange(predictions.shape[1])).astype(int)\n",
    "    accuracy = 1 - np.sum(np.abs(_predictions - y))/(2*len(y))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "w2v\n",
      "mlp\n",
      "1 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 225us/step - loss: 0.6319 - acc: 0.7868\n",
      "2 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 212us/step - loss: 0.2705 - acc: 0.9105\n",
      "3 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 215us/step - loss: 0.2323 - acc: 0.9214\n",
      "4 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 216us/step - loss: 0.2169 - acc: 0.9251\n",
      "5 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 226us/step - loss: 0.2067 - acc: 0.9277\n",
      "6 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 238us/step - loss: 0.1990 - acc: 0.9313\n",
      "7 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 241us/step - loss: 0.1943 - acc: 0.9337\n",
      "8 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 241us/step - loss: 0.1901 - acc: 0.9346\n",
      "9 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 240us/step - loss: 0.1859 - acc: 0.9376\n",
      "10 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 242us/step - loss: 0.1817 - acc: 0.9367\n",
      "11 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 240us/step - loss: 0.1790 - acc: 0.9400\n",
      "12 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 242us/step - loss: 0.1747 - acc: 0.9397\n",
      "13 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 242us/step - loss: 0.1724 - acc: 0.9403\n",
      "14 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 248us/step - loss: 0.1694 - acc: 0.9418\n",
      "15 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 241us/step - loss: 0.1663 - acc: 0.9422\n",
      "16 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 243us/step - loss: 0.1654 - acc: 0.9431\n",
      "17 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 249us/step - loss: 0.1627 - acc: 0.9426\n",
      "18 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 243us/step - loss: 0.1611 - acc: 0.9438\n",
      "19 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 244us/step - loss: 0.1601 - acc: 0.9435\n",
      "20 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 247us/step - loss: 0.1580 - acc: 0.9445\n",
      "1\n",
      "w2v\n",
      "mlp\n",
      "1 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 260us/step - loss: 0.4621 - acc: 0.8495\n",
      "2 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 248us/step - loss: 0.2443 - acc: 0.9210\n",
      "3 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 249us/step - loss: 0.2223 - acc: 0.9274\n",
      "4 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 252us/step - loss: 0.2117 - acc: 0.9316\n",
      "5 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 254us/step - loss: 0.2046 - acc: 0.9323\n",
      "6 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 293us/step - loss: 0.1990 - acc: 0.9345\n",
      "7 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 292us/step - loss: 0.1953 - acc: 0.9343\n",
      "8 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 292us/step - loss: 0.1909 - acc: 0.9366\n",
      "9 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 293us/step - loss: 0.1882 - acc: 0.9381\n",
      "10 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 293us/step - loss: 0.1859 - acc: 0.9383\n",
      "11 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 290us/step - loss: 0.1824 - acc: 0.9378\n",
      "12 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 288us/step - loss: 0.1797 - acc: 0.9407\n",
      "13 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 284us/step - loss: 0.1779 - acc: 0.9409\n",
      "14 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 285us/step - loss: 0.1762 - acc: 0.9406\n",
      "15 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 287us/step - loss: 0.1736 - acc: 0.9428\n",
      "16 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 287us/step - loss: 0.1718 - acc: 0.9430\n",
      "17 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 284us/step - loss: 0.1706 - acc: 0.9423\n",
      "18 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 283us/step - loss: 0.1695 - acc: 0.9426\n",
      "19 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 279us/step - loss: 0.1678 - acc: 0.9444\n",
      "20 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 297us/step - loss: 0.1677 - acc: 0.9427\n",
      "2\n",
      "w2v\n",
      "mlp\n",
      "1 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 266us/step - loss: 0.5265 - acc: 0.8148\n",
      "2 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 251us/step - loss: 0.2678 - acc: 0.9118\n",
      "3 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 250us/step - loss: 0.2295 - acc: 0.9247\n",
      "4 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 253us/step - loss: 0.2133 - acc: 0.9318\n",
      "5 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 253us/step - loss: 0.2026 - acc: 0.9338\n",
      "6 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 284us/step - loss: 0.1965 - acc: 0.9366\n",
      "7 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 265us/step - loss: 0.1905 - acc: 0.9376\n",
      "8 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 277us/step - loss: 0.1854 - acc: 0.9386\n",
      "9 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 273us/step - loss: 0.1821 - acc: 0.9388\n",
      "10 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 288us/step - loss: 0.1794 - acc: 0.9403\n",
      "11 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 277us/step - loss: 0.1763 - acc: 0.9403\n",
      "12 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 275us/step - loss: 0.1740 - acc: 0.9396\n",
      "13 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 271us/step - loss: 0.1722 - acc: 0.9414\n",
      "14 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 272us/step - loss: 0.1697 - acc: 0.9424\n",
      "15 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 277us/step - loss: 0.1682 - acc: 0.9419\n",
      "16 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 272us/step - loss: 0.1660 - acc: 0.9431\n",
      "17 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 269us/step - loss: 0.1658 - acc: 0.9427\n",
      "18 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 270us/step - loss: 0.1646 - acc: 0.9421\n",
      "19 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 267us/step - loss: 0.1629 - acc: 0.9441\n",
      "20 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 265us/step - loss: 0.1616 - acc: 0.9452\n",
      "3\n",
      "w2v\n",
      "mlp\n",
      "1 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 287us/step - loss: 0.4555 - acc: 0.8488\n",
      "2 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 271us/step - loss: 0.2438 - acc: 0.9198\n",
      "3 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 267us/step - loss: 0.2193 - acc: 0.9281\n",
      "4 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 269us/step - loss: 0.2064 - acc: 0.9320\n",
      "5 Epoch 1/1\n",
      "16964/16964 [==============================] - 5s 265us/step - loss: 0.1991 - acc: 0.9324\n",
      "6 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 264us/step - loss: 0.1920 - acc: 0.9350\n",
      "7 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 261us/step - loss: 0.1875 - acc: 0.9355\n",
      "8 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 263us/step - loss: 0.1838 - acc: 0.9385\n",
      "9 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 262us/step - loss: 0.1818 - acc: 0.9374\n",
      "10 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 263us/step - loss: 0.1797 - acc: 0.9380\n",
      "11 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 260us/step - loss: 0.1770 - acc: 0.9393\n",
      "12 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 260us/step - loss: 0.1746 - acc: 0.9393\n",
      "13 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 261us/step - loss: 0.1737 - acc: 0.9401\n",
      "14 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 258us/step - loss: 0.1715 - acc: 0.9418\n",
      "15 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 259us/step - loss: 0.1692 - acc: 0.9412\n",
      "16 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 259us/step - loss: 0.1689 - acc: 0.9435\n",
      "17 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 257us/step - loss: 0.1674 - acc: 0.9446\n",
      "18 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 255us/step - loss: 0.1659 - acc: 0.9412\n",
      "19 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 254us/step - loss: 0.1650 - acc: 0.9431\n",
      "20 Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16964/16964 [==============================] - 4s 208us/step - loss: 0.1633 - acc: 0.9434\n",
      "4\n",
      "w2v\n",
      "mlp\n",
      "1 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 184us/step - loss: 0.5022 - acc: 0.8410\n",
      "2 Epoch 1/1\n",
      "16964/16964 [==============================] - 4s 210us/step - loss: 0.2418 - acc: 0.9195\n",
      "3 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 173us/step - loss: 0.2191 - acc: 0.9256\n",
      "4 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 172us/step - loss: 0.2069 - acc: 0.9314\n",
      "5 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 165us/step - loss: 0.1973 - acc: 0.9349\n",
      "6 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 167us/step - loss: 0.1923 - acc: 0.9350\n",
      "7 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 164us/step - loss: 0.1876 - acc: 0.9369\n",
      "8 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 167us/step - loss: 0.1832 - acc: 0.9386\n",
      "9 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 166us/step - loss: 0.1800 - acc: 0.9373\n",
      "10 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 167us/step - loss: 0.1763 - acc: 0.9388\n",
      "11 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 165us/step - loss: 0.1736 - acc: 0.9396\n",
      "12 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 165us/step - loss: 0.1711 - acc: 0.9390\n",
      "13 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 167us/step - loss: 0.1684 - acc: 0.9421\n",
      "14 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 164us/step - loss: 0.1665 - acc: 0.9419\n",
      "15 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 165us/step - loss: 0.1651 - acc: 0.9440\n",
      "16 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 166us/step - loss: 0.1625 - acc: 0.9438\n",
      "17 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 164us/step - loss: 0.1610 - acc: 0.9454\n",
      "18 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 164us/step - loss: 0.1607 - acc: 0.9448\n",
      "19 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 170us/step - loss: 0.1582 - acc: 0.9455\n",
      "20 Epoch 1/1\n",
      "16964/16964 [==============================] - 3s 171us/step - loss: 0.1569 - acc: 0.9463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "mlp_accuracy = []\n",
    "no_of_outputs = 7\n",
    "\n",
    "for i,X_train in enumerate(X_trains):\n",
    "    print(i)\n",
    "    y_train = y_trains[i]\n",
    "    y_test = y_tests[i]\n",
    "    X_test = X_tests[i]\n",
    "    \n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(y_train)\n",
    "\n",
    "    y_train_t = encoder.transform(y_train)\n",
    "    y_test_t = encoder.transform(y_test)\n",
    "\n",
    "    texts_train = get_texts(X_train)\n",
    "    texts_test  = get_texts(X_test)\n",
    "\n",
    "    print(\"w2v\")\n",
    "    model = Word2Vec(texts_train, size=100, window=4, min_count=1)\n",
    "    model.train(texts_train, total_examples=len(texts_train), epochs=50)\n",
    "    model.wv.save(\"trained_word_vectors_train.pkl\")\n",
    "    word_vectors = KeyedVectors.load(\"trained_word_vectors_train.pkl\")\n",
    "\n",
    "    averaged_train,_ = get_averaged_doc(texts_train)\n",
    "    averaged_test, new_words = get_averaged_doc(texts_test)\n",
    "\n",
    "    print(\"mlp\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=100, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(no_of_outputs, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for i in range(20):    \n",
    "        print(i+1,end=\" \")\n",
    "        test_accuracies.append(get_accuracy(averaged_test,model,y_test_t))\n",
    "        train_accuracies.append(get_accuracy(averaged_train,model,y_train_t))\n",
    "\n",
    "        model.fit(averaged_train, y_train_t, epochs=1, batch_size=10)\n",
    "        \n",
    "    mlp_accuracy.append(test_accuracies[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(test_accuracies[1:], 'r')\n",
    "plt.plot(train_accuracies[1:], 'b')\n",
    "plt.legend([\"test\",\"train\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9327517095024758"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mlp_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3,\n",
    "                                            max_iter=5, tol=None)),])\n",
    "\n",
    "svm_accuracies = []\n",
    "\n",
    "for i in range(5):\n",
    "    x_train = [' '.join(x) for x in X_trains[i]]\n",
    "    y_train = y_trains[i]\n",
    "    x_test  = [' '.join(x) for x in X_tests[i]]\n",
    "    y_test  = y_tests[i]\n",
    "    text_clf.fit(x_train, y_train)\n",
    "    predicted = text_clf.predict(x_test)\n",
    "    svm_accuracies.append(np.mean(predicted == y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9236029238387173,\n",
       " 0.932563074746522,\n",
       " 0.9297335534072153,\n",
       " 0.930440933742042,\n",
       " 0.9316199009667532]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92959207734025"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(svm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
